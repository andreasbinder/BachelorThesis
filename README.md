# Preference-based Inverse Reinforcement Learning Approach for Robot Locomotion under Suboptimal Learning Conditions
**Bachelor's Thesis in Robotics**

## Abstract

Recent improvements in Reinforcement Learning (RL) have gained huge attention by both the media and academia. Additionally, with Neural Networks being universal approximators, it is possible to model complex reward functions which used to be intractable. Especially policy-gradient algorithms are popular among researchers because they can be applied on continuous, high-dimensional tasks, most notably OpenAI's Proximal Policy Optimization, which will be also used for this work. Policy-gradient methods have better convergence properties than traditional RL algorithms and can model the true distribution of actions quite well when provided with a proper reward function. This is the reason where the paradigm of Inverse Reinforcement Learning (IRL) comes into play. Given a policy, IRL tries to infer the underlying reward function. Most IRL algorithms suffer from the problem of not being able to extrapolate beyond their provided policy. This work aims to tackle this problem by using trajectories generated by a suboptimal policy which is used to deliver far better results compared to other state-of-the art approaches. We build on Daniel Brown's work on T-REX, which uses pairwise trajectory ranking to capture the intent of the agent and thus obtains very good results on standard Mujoco tasks and Atari games. We additionally investigate the impact of the loss function. Specifically, we analyze whether there are performance gains when choosing standard ranking loss that operates on a list of trajectories. Pair-wise ranking is only a special case of the list-wise ranking where the number of trajectories is set to two. 
The motivation to use suboptimal trajectories reflects the fact that generally they are easier to obtain and can be derived for most tasks relatively cheaply. In addition, it can be argued that the reward function is of higher quality since it mirrors the intention instead of plainly trying to explain the presented trajectories. 
We want to implement those ideas for a snake-like robot that can be simulated on the Mujoco environment. Those autonomous robots are used in catastrophic situation in terrain that is hardly accessible to human beings. For instance, it can be made use of at collapsed houses after an earthquake where it can help finding buried humans.

## Setup


### System:
- Linux Mint 19.2 
- Python 3.6.8 (Note: anything higher than Ubuntu 16.4 does the job as well)
- I recommend using a virtual python environment

### Requirements:

Please refer to the requirements.txt file for the dependencies (Note: Not all requirements are needed, there have to be done some checks of what is indispensable)
Especially for setting up the snake mujoco environment, have a look at [this README file](README_Lemke.md). 

```bash
    pip install -r requirements.txt
```

## Usage

### Short Description

This project consists of three major steps to perform IRL. 

1. run PPO on the ground truth and get trajectories (list of observations) and the cumulative reward for that horizon (can be done online or offline). The default (offline) dataset is stored [here](gym_mujoco_planar_snake/gym_mujoco_planar_snake/log/TrajectoryDataset) which is generated with [this file](gym_mujoco_planar_snake/gym_mujoco_planar_snake/agents/run_ppo_offline.py) followed by [generate_trajectories.py](gym_mujoco_planar_snake/gym_mujoco_planar_snake/agents/generate_trajectories.py) (This procedure includes saving everything generated by the PPO run to the disk and then using generate_trajectories.py to get the trainable data. It will soon be replaced by the following). The main file is found [this file](gym_mujoco_planar_snake/gym_mujoco_planar_snake/agents/run_ppo.py) (I aim to include the online trajectory generation there, still in test phase).

2. Depending on the used loss function, we preprocess the dataset, eg. form pairs for binary crossentropy and triplets for triplet loss. [learn_reward.py](gym_mujoco_planar_snake/gym_mujoco_planar_snake/agents/learn_reward.py) is responsible for telling the [trainer](gym_mujoco_planar_snake/gym_mujoco_planar_snake/common/trainer.py) class to run the right algorithm. It also points to the location of the json file [hparams](gym_mujoco_planar_snake/gym_mujoco_planar_snake/agents/hparams.json) containing the hyperparameters. 

3. Lastly, you run [run_ppo.py](gym_mujoco_planar_snake/gym_mujoco_planar_snake/agents/run_ppo.py) again but now using the learnt reward function. [reward_wrapper_pytorch](gym_mujoco_planar_snake/gym_mujoco_planar_snake/common/reward_wrapper_pytorch.py) is responsible for changing the ground truth reward for the predicted reward.

### Script Templates

All commands are run inside the [gym_mujoco_planar_snake](gym_mujoco_planar_snake) directory.

Run PPO

```bash
    python3 gym_mujoco_planar_snake/agents/run_ppo.py --sfs 50000 --num-timesteps 1e6 --log_dir gym_mujoco_planar_snake/log/initial_PPO_runs/ --custom_reward False
```

If you want to have truly inferior trajectories, you can specify the arguments:
```bash
    --clip_value 1.5 --fixed_joints 0
```
--clipvalue specifying the value you want to actions to be at max and --fixed_joints specifying the joints having that impediment (index ranges from 0 to 7 as there exist 8 joints)

Learn Reward

```bash
    python3 gym_mujoco_planar_snake/agents/learn_reward.py --mode pair --data_dir gym_mujoco_planar_snake/log/SubTrajectoryDataset --net_save_path gym_mujoco_planar_snake/log/PyTorch_Models/ --hparams_path gym_mujoco_planar_snake/agents/hparams.json

```
Run PPO with custom reward

```bash
    python3 gym_mujoco_planar_snake/agents/run_ppo.py --sfs 50000 --num-timesteps 1e6 --log_dir gym_mujoco_planar_snake/log/improved_PPO_runs/ --custom_reward True
```

## Author
Andreas Binder, Information Systems (B.Sc.), Technical University Munich


