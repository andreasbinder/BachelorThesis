# TODO
# Optional: you can download the data instead
create_dataset:
  seed: 0
  variable_scope: 0
  # learning
  save_frequency:  5000
  num_timesteps: 1500000
  checkpoint_dir: /tmp/checkpoints/
  data_dir: /tmp/data/

  # TODO actually part of main
  # dataset creation
  subtrajector_length: 50 #50 100
  episode_length: 1000
  percentage: 0.33



train:
  seed: 0
  num_nets: 1
  input_dim: 27
  split_ratio: 0.9
  # available
  ranking_method: triplet
  hparams:
    lr: 0.00005 #0.00005
    # pair 2, 15
    epochs: 15 #10, 20 for triplet cum_reward
    ranking_approach: 5
    dataset_size: 0



evaluate: TODO

extrapolation:
  seed: 0
  train_path: /home/andreas/Desktop/2020-10-03_00-13-18_0.5/train.npy
  extrapolation_path: /home/andreas/Desktop/2020-10-03_00-13-18_0.5/extrapolate.npy
  net_path: /tmp/model_0

visualize:
  seed: 0
  variable_scope: 1
  model_dir: /tmp/pi_hat/2020-10-03_17-10-16/1


# define hyperparameters for IRL training
reward_learning:
  num_nets: 1
  input_dim: 27
  split_ratio: 0.9
  # available
  ranking_method: triplet
  hparams:
    lr: 0.00005 #0.00005
    # pair 2, 15
    epochs: 20 #10, 20 for triplet cum_reward
    ranking_approach: 5
    dataset_size: 0

ppo:
  general:
    env_id: Mujoco-planar-snake-cars-angle-line-v1
    seed: 0
    log_dir: src/results/
  initial:
    create_trajectories: false # if false you need to provide training data
    trajectory_length: 100 #50
    num_traj_per_episode: 10
    num_agents: 1 # 5
    num_timesteps: 3000 # 1000000 1500000
    sfs: 1000 #10000 5000
    save_checkpoints: true
  improved:
    validate_learned_reward: false
    num_agents: 1
    num_timesteps: 1000000 #1000000
    sfs: 100000 #100000
    save_checkpoints: true
    ctrl_coeff: 0.0 #0.0  should prevent agent from taking too high action values


